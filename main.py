# import numpy as np
# from tqdm import tqdm
# import os
# from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor
# import torch
# import torch.nn as nn
# import time
# import importlib
# import wandb
# from models.reward.models.modeling import SpeechMeshTransformer
# from src import utils
# from models.reward.models.head_v2 import ScoreHead
# from functools import partial
# from omegaconf import OmegaConf, DictConfig
# import hydra



# def count_parameters(model):
#     return sum(p.numel() for p in model.parameters() if p.requires_grad)


# @hydra.main(version_base=None, config_path="configs", config_name="config")
# def main(cfg: DictConfig):

#     model_name = cfg.model.name
#     loader_name = cfg.dataset.loader_name
#     model_module = importlib.import_module(f'models.{model_name}.model')
#     ModelClass = getattr(model_module, 'Model')
#     trainer_module = importlib.import_module(f'trainer.{model_name}.trainer')
#     dataset_loader_module = importlib.import_module(f'dataset.{loader_name}')
#     get_dataloaders = getattr(dataset_loader_module, 'get_dataloaders')

    
#     model = ModelClass(cfg, cfg.dataset.name, cfg.trainer.device)
#     print("model parameters: ", count_parameters(model))

#     assert torch.cuda.is_available()
#     model = model.to(cfg.trainer.device)

#     ## Perceptual-3D-Talking-Head
#     guidance_model = SpeechMeshTransformer(
#         vertex_size=5023 * 3,
#         patch_size=5023 * 3,
#         embed_dim=512,
#         num_heads=8,
#         depth=10,
#         # audio
#         img_size_audio=(64, 128),  # (T, F)
#         patch_size_audio=16,
#         embed_dim_audio=512,
#         num_heads_audio=8,
#         mlp_ratio=4,
#         qkv_bias=True,
#         norm_layer=partial(nn.LayerNorm, eps=1e-6),
#         max_length=5,
#         depth_audio=10,)

#     guidance_model_path = OmegaConf.select(cfg, "reward.guidance_model_path")
#     if guidance_model_path is None:
#         guidance_model_path = cfg.trainer.guidance_model_path

#     print("Load guidance model ckpt from %s" % guidance_model_path)
#     checkpoint = torch.load(guidance_model_path, map_location='cpu', weights_only=False)

#     checkpoint_model = checkpoint['model']
#     utils.load_state_dict(guidance_model, checkpoint_model)

#     head = ScoreHead(d_audio=512, d_mesh=512,
#                     hidden=256, dropout=0.1,
#                     out_activation='sigmoid').cuda()

#     guidance_head_model_path = OmegaConf.select(cfg, "reward.guidance_head_model_path")
#     if guidance_head_model_path is None:
#         guidance_head_model_path = cfg.trainer.guidance_head_model_path

#     checkpoint = torch.load(guidance_head_model_path, map_location='cpu', weights_only=False)


#     head_checkpoint = checkpoint['head']
#     head.load_state_dict(head_checkpoint)

#     for name, param in guidance_model.named_parameters():
#         param.requires_grad = False
    
#     guidance_model.to(torch.device("cuda"))
#     guidance_model.eval()

#     for name, param in head.named_parameters():
#         param.requires_grad = False

#     head.to(torch.device("cuda"))
#     head.eval()

#     # load data
#     dataset = get_dataloaders(cfg.dataset)
#     # loss
#     if cfg.train:
#         criterion = nn.MSELoss()

#         optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.trainer.lr)
#         trainer_module.train(cfg, dataset["train"], dataset["valid"], model, guidance_model, head, optimizer, criterion, epoch=cfg.trainer.max_epoch,
#                 last_train=cfg.trainer.last_train)
#     if cfg.test:
#         # trainer_module.test(cfg, model, dataset["test"])
#         trainer_module.test_styleindependant(cfg, model, dataset["test"])
#         # try:
#         #     trainer_module.test_styleindependant(cfg, model, dataset["test"])
#         # except Exception as e:
#         #     print("this model could Just Test Style dependant: ", e)


# if __name__ == "__main__":
#     main()
import numpy as np
from tqdm import tqdm
import os
from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor
import torch
import torch.nn as nn
import time
import importlib
import wandb
from models.reward.models.modeling import SpeechMeshTransformer
from src import utils
from models.reward.models.head_v2 import ScoreHead
from functools import partial
from omegaconf import OmegaConf, DictConfig
import hydra


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):

    # ----------------------------------------------------------------------
    # 0. Hydraë¡œ ëª¨ë¸ / ë°ì´í„°ë¡œë” / íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ ë¡œë”©
    # ----------------------------------------------------------------------
    model_name = cfg.model.name
    loader_name = cfg.dataset.loader_name

    model_module = importlib.import_module(f'models.{model_name}.model')
    ModelClass = getattr(model_module, 'Model')

    trainer_module = importlib.import_module(f'trainer.{model_name}.trainer')
    dataset_loader_module = importlib.import_module(f'dataset.{loader_name}')
    get_dataloaders = getattr(dataset_loader_module, 'get_dataloaders')

    device = cfg.trainer.device

    # ----------------------------------------------------------------------
    # 1. Actor: FaceFormer (baseline ëª¨ë¸)
    # ----------------------------------------------------------------------
    model = ModelClass(cfg, cfg.dataset.name, device)
    print("model parameters: ", count_parameters(model))

    assert torch.cuda.is_available()
    model = model.to(device)

    # ----------------------------------------------------------------------
    # 2. Reward Backbone: SpeechMeshTransformer (Perceptual 3D Talking Head)
    #    - freeze í•´ì„œ feature extractor / í‰ê°€ì ì—­í• ë§Œ ìˆ˜í–‰
    # ----------------------------------------------------------------------
    guidance_model = SpeechMeshTransformer(
        vertex_size=5023 * 3,
        patch_size=5023 * 3,
        embed_dim=512,
        num_heads=8,
        depth=10,
        # audio
        img_size_audio=(64, 128),  # (T, F)
        patch_size_audio=16,
        embed_dim_audio=512,
        num_heads_audio=8,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        max_length=5,
        depth_audio=10,
    )

    guidance_model_path = OmegaConf.select(cfg, "reward.guidance_model_path")
    if guidance_model_path is None:
        guidance_model_path = cfg.trainer.guidance_model_path

    print("Load guidance model ckpt from %s" % guidance_model_path)
    checkpoint = torch.load(guidance_model_path, map_location='cpu', weights_only=False)
    checkpoint_model = checkpoint['model']
    utils.load_state_dict(guidance_model, checkpoint_model)

    # backboneì€ ê³ ì • (freeze)
    for name, param in guidance_model.named_parameters():
        param.requires_grad = False

    guidance_model = guidance_model.to(device)
    guidance_model.eval()

    # ----------------------------------------------------------------------
    # 3. ScoreHead: Reward(+Critic Value) Head
    #    - lip_score / real_score / value(V(s))ë¥¼ ê°™ì´ ë½‘ëŠ” MLP
    # ----------------------------------------------------------------------
    head = ScoreHead(
        d_audio=512,
        d_mesh=512,
        hidden=256,
        dropout=0.1,
        out_activation='sigmoid'
    ).to(device)

    guidance_head_model_path = OmegaConf.select(cfg, "reward.guidance_head_model_path")
    if guidance_head_model_path is None:
        guidance_head_model_path = cfg.trainer.guidance_head_model_path

    print("Load guidance head ckpt from %s" % guidance_head_model_path)
    checkpoint = torch.load(guidance_head_model_path, map_location='cpu', weights_only=False)

    head_checkpoint = checkpoint['head']
    # value headê°€ ìƒˆë¡œ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ strict=Falseë¡œ ë¡œë”© (ì—†ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”)
    head.load_state_dict(head_checkpoint, strict=False)

    # ğŸ”¥ HeadëŠ” RL í•™ìŠµì— ì°¸ì—¬í•´ì•¼ í•˜ë¯€ë¡œ requires_grad=True ìœ ì§€
    # (ì´ë¯¸ ê¸°ë³¸ê°’ì´ Trueë¼ì„œ ë”°ë¡œ ê±´ë“œë¦´ í•„ìš”ëŠ” ì—†ìŒ)
    head.train()

    # ----------------------------------------------------------------------
    # 4. ë°ì´í„° ë¡œë” ì¤€ë¹„
    # ----------------------------------------------------------------------
    dataset = get_dataloaders(cfg.dataset)
    train_loader = dataset["train"]
    valid_loader = dataset["valid"]
    test_loader = dataset["test"]

    # ----------------------------------------------------------------------
    # 5. Loss & Optimizer ì„¤ì •
    #    - Actor(model) + Head(head)ë¥¼ í•¨ê»˜ ì—…ë°ì´íŠ¸
    #    - Reward Backbone(guidance_model)ì€ ê³ ì •
    # ----------------------------------------------------------------------
    if cfg.train:
        # FaceFormerì˜ vertex íšŒê·€ëŠ” L1/MSE ì¤‘ í•˜ë‚˜ ì‚¬ìš© ê°€ëŠ¥
        # ê¸°ì¡´ ì½”ë“œ ê¸°ì¤€ìœ¼ë¡œ MSELoss ì‚¬ìš©
        criterion = nn.MSELoss(reduction='none')

        # Actor + Head íŒŒë¼ë¯¸í„°ë¥¼ ë¶„ë¦¬í•´ lrì„ ë‹¤ë¥´ê²Œ ì¤„ ìˆ˜ ìˆê²Œ êµ¬ì„±
        actor_params = list(filter(lambda p: p.requires_grad, model.parameters()))
        head_params = list(filter(lambda p: p.requires_grad, head.parameters()))
        optimizer = torch.optim.Adam(
            [
                {"params": actor_params, "lr": cfg.trainer.lr},
                {"params": head_params, "lr": cfg.trainer.head_lr},
            ]
        )

        # RL loss weight ê¸°ë³¸ê°’ ìë™ ì„¸íŒ… (configì— ì—†ìœ¼ë©´)
        actor_w = OmegaConf.select(cfg, "trainer.actor_weight")
        critic_w = OmegaConf.select(cfg, "trainer.critic_weight")
        if actor_w is None:
            cfg.trainer.actor_weight = 1e-4
        if critic_w is None:
            cfg.trainer.critic_weight = 1e-4

        trainer_module.train(
            cfg,
            train_loader,
            valid_loader,
            model,
            guidance_model,
            head,
            optimizer,
            criterion,
            epoch=cfg.trainer.max_epoch,
            last_train=cfg.trainer.last_train,
        )

    # ----------------------------------------------------------------------
    # 6. í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ êµ¬ì¡° ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    # ----------------------------------------------------------------------
    if cfg.test:
        # style dependant
        trainer_module.test(cfg, model, test_loader)

        # style independant
        #trainer_module.test_styleindependant(cfg, model, test_loader)
        # í•„ìš”í•˜ë©´ try/exceptë¡œ ë‘ ì¢…ë¥˜ í…ŒìŠ¤íŠ¸ë¥¼ ë‚˜ëˆŒ ìˆ˜ë„ ìˆìŒ

if __name__ == "__main__":
    main()
